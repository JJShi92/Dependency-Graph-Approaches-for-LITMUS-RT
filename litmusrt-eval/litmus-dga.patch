diff -Naur litmus-rt/drivers/media/usb/as102/as102_usb_drv.c litmus-rt-dga/drivers/media/usb/as102/as102_usb_drv.c
--- litmus-rt/drivers/media/usb/as102/as102_usb_drv.c	2018-05-27 18:16:12.270921840 +0200
+++ litmus-rt-dga/drivers/media/usb/as102/as102_usb_drv.c	2018-05-27 18:18:39.158825597 +0200
@@ -114,7 +114,7 @@
 
 	if (recv_buf != NULL) {
 #ifdef TRACE
-		dev_dbg(bus_adap->usb_dev->dev,
+		dev_dbg(&bus_adap->usb_dev->dev,
 			"want to read: %d bytes\n", recv_buf_len);
 #endif
 		ret = usb_control_msg(bus_adap->usb_dev,
@@ -132,7 +132,7 @@
 			return ret;
 		}
 #ifdef TRACE
-		dev_dbg(bus_adap->usb_dev->dev,
+		dev_dbg(&bus_adap->usb_dev->dev,
 			"read %d bytes\n", recv_buf_len);
 #endif
 	}
diff -Naur litmus-rt/include/litmus/fdso.h litmus-rt-dga/include/litmus/fdso.h
--- litmus-rt/include/litmus/fdso.h	2018-05-27 18:17:05.366883601 +0200
+++ litmus-rt-dga/include/litmus/fdso.h	2018-05-27 16:40:02.000000000 +0200
@@ -26,8 +26,9 @@
 	PCP_SEM         = 5,
 
 	DFLP_SEM	= 6,
-
-	MAX_OBJ_TYPE	= 6
+	SDGA_SEM	= 7,
+	PDGA_SEM	= 8,
+	MAX_OBJ_TYPE	= 8
 } obj_type_t;
 
 struct inode_obj_id {
diff -Naur litmus-rt/include/litmus/litmus.h litmus-rt-dga/include/litmus/litmus.h
--- litmus-rt/include/litmus/litmus.h	2018-05-27 18:17:05.366883601 +0200
+++ litmus-rt-dga/include/litmus/litmus.h	2018-05-27 15:14:21.000000000 +0200
@@ -29,6 +29,7 @@
 }
 
 struct task_struct* __waitqueue_remove_first(wait_queue_head_t *wq);
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq);
 
 #define NO_CPU			0xffffffff
 
@@ -72,6 +73,9 @@
 #define get_rt_phase(t)		(tsk_rt(t)->task_params.phase)
 #define get_partition(t) 	(tsk_rt(t)->task_params.cpu)
 #define get_priority(t) 	(tsk_rt(t)->task_params.priority)
+#define get_rt_order(t)		(tsk_rt(t)->task_params.rt_order)
+#define get_rt_total(t)		(tsk_rt(t)->task_params.total_tasks)
+
 #define get_class(t)        (tsk_rt(t)->task_params.cls)
 #define get_release_policy(t) (tsk_rt(t)->task_params.release_policy)
 
diff -Naur litmus-rt/include/litmus/rt_param.h litmus-rt-dga/include/litmus/rt_param.h
--- litmus-rt/include/litmus/rt_param.h	2018-05-27 18:17:05.370883598 +0200
+++ litmus-rt-dga/include/litmus/rt_param.h	2018-05-27 13:49:18.000000000 +0200
@@ -122,6 +122,8 @@
 	task_class_t	cls;
 	budget_policy_t  budget_policy;  /* ignored by pfair */
 	release_policy_t release_policy;
+	unsigned int	rt_order;
+	unsigned int 	total_tasks;
 };
 
 /* don't export internal data structures to user space (liblitmus) */
diff -Naur litmus-rt/include/litmus/wait.h litmus-rt-dga/include/litmus/wait.h
--- litmus-rt/include/litmus/wait.h	2018-05-27 18:17:05.370883598 +0200
+++ litmus-rt-dga/include/litmus/wait.h	2018-05-27 13:55:30.000000000 +0200
@@ -2,6 +2,7 @@
 #define _LITMUS_WAIT_H_
 
 struct task_struct* __waitqueue_remove_first(wait_queue_head_t *wq);
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq);
 
 /* wrap regular wait_queue_t head */
 struct __prio_wait_queue {
diff -Naur litmus-rt/litmus/fdso.c litmus-rt-dga/litmus/fdso.c
--- litmus-rt/litmus/fdso.c	2018-05-27 18:17:05.378883593 +0200
+++ litmus-rt-dga/litmus/fdso.c	2018-05-27 16:41:30.000000000 +0200
@@ -28,6 +28,8 @@
 	&generic_lock_ops, /* DPCP_SEM */
 	&generic_lock_ops, /* PCP_SEM */
 	&generic_lock_ops, /* DFLP_SEM */
+	&generic_lock_ops, /* SDGA_SEM */
+	&generic_lock_ops, /* PDGA_SEM */
 };
 
 static int fdso_create(void** obj_ref, obj_type_t type, void* __user config)
diff -Naur litmus-rt/litmus/locking.c litmus-rt-dga/litmus/locking.c
--- litmus-rt/litmus/locking.c	2018-05-27 18:17:05.378883593 +0200
+++ litmus-rt-dga/litmus/locking.c	2018-05-27 14:04:42.000000000 +0200
@@ -138,6 +138,18 @@
 	return(t);
 }
 
+struct task_struct* __check_waitqueue_first(wait_queue_head_t *wq)
+{
+	wait_queue_t* q;
+	struct task_struct* t = NULL;
+	
+	if (waitqueue_active(wq)) {
+		q = list_entry(wq->task_list.next, wait_queue_t, task_list);
+		t = (struct task_struct*) q->private;
+	}
+	return(t);
+}
+
 unsigned int __add_wait_queue_prio_exclusive(
 	wait_queue_head_t* head,
 	prio_wait_queue_t *new)
diff -Naur litmus-rt/litmus/sched_pfp.c litmus-rt-dga/litmus/sched_pfp.c
--- litmus-rt/litmus/sched_pfp.c	2018-05-27 18:17:05.382883590 +0200
+++ litmus-rt-dga/litmus/sched_pfp.c	2018-05-27 17:36:06.000000000 +0200
@@ -1591,6 +1591,487 @@
 	return &sem->litmus_lock;
 }
 
+/* ******************** PDGA support ********************** */
+
+struct pdga_semaphore {
+        struct litmus_lock litmus_lock;
+
+        /* current resource holder */
+        struct task_struct *owner;
+
+        /* priority queue of waiting tasks */
+        wait_queue_head_t wait;
+
+        /* priority ceiling per cpu */
+        unsigned int prio_ceiling[NR_CPUS];
+
+	/* current serving task */
+        atomic_t serving_ticket;
+};
+
+static inline struct pdga_semaphore* pdga_from_lock(struct litmus_lock* lock)
+{
+        return container_of(lock, struct pdga_semaphore, litmus_lock);
+}
+
+int pfp_pdga_lock(struct litmus_lock* l)
+{
+        struct task_struct* t = current;
+        struct pdga_semaphore *sem = pdga_from_lock(l);
+	/* In DGA, ordered by the execution order */
+        prio_wait_queue_t wait;
+        unsigned long flags;
+
+	unsigned int current_serving_ticket;
+	unsigned int task_order;
+
+        if (!is_realtime(t))
+                return -EPERM;
+
+        /* prevent nested lock acquisition */
+        if (tsk_rt(t)->num_locks_held ||
+            tsk_rt(t)->num_local_locks_held)
+                return -EBUSY;
+
+        preempt_disable();
+
+        /* Priority-boost ourself *before* we suspend so that
+         * our priority is boosted when we resume. Use the priority
+         * ceiling for the local partition. */
+        boost_priority(t, sem->prio_ceiling[get_partition(t)]);
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+
+        preempt_enable_no_resched();
+
+	current_serving_ticket = atomic_read(&sem->serving_ticket);
+	task_order = get_rt_order(t);
+	
+	TRACE_CUR("Lock: task order is now %d and semaphore serving is %d on processor %d\n", task_order, current_serving_ticket, get_partition(t));
+
+        if (sem->owner || current_serving_ticket != task_order) {
+                /* resource is not free => must suspend and wait */
+
+                /* ordered by regular priority */
+                init_prio_waitqueue_entry(&wait, t, prio_point(get_rt_order(t)));
+
+                /* FIXME: interruptible would be nice some day */
+                set_task_state(t, TASK_UNINTERRUPTIBLE);
+
+                __add_wait_queue_prio_exclusive(&sem->wait, &wait);
+
+                TS_LOCK_SUSPEND;
+
+                /* release lock before sleeping */
+                spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+                /* We depend on the FIFO order.  Thus, we don't need to recheck
+                 * when we wake up; we are guaranteed to have the lock since
+                 * there is only one wake up per release.
+                 */
+
+                schedule();
+
+                TS_LOCK_RESUME;
+
+                /* Since we hold the lock, no other task will change
+                 * ->owner. We can thus check it without acquiring the spin
+                 * lock. */
+                BUG_ON(sem->owner != t);
+        } else {
+                /* it's ours now */
+                sem->owner = t;
+
+                spin_unlock_irqrestore(&sem->wait.lock, flags);
+        }
+
+        tsk_rt(t)->num_locks_held++;
+
+        return 0;
+}
+
+int pfp_pdga_unlock(struct litmus_lock* l)
+{
+        struct task_struct *t = current;
+	struct task_struct *next = NULL;
+	struct task_struct *next_candi = NULL;
+        struct pdga_semaphore *sem = pdga_from_lock(l);
+        unsigned long flags;
+        int err = 0;
+
+	unsigned int next_serving_ticket = -1;
+	unsigned int task_order = -1;
+	unsigned int max_serving_ticket = -1;
+
+        preempt_disable();
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+
+        if (sem->owner != t) {
+                err = -EINVAL;
+                goto out;
+        }
+
+	max_serving_ticket = get_rt_total(t) - 1;
+        tsk_rt(t)->num_locks_held--;
+
+	atomic_add(1, &sem->serving_ticket);
+	next_serving_ticket = atomic_read(&sem->serving_ticket);
+
+	if (next_serving_ticket > max_serving_ticket) {
+		next_serving_ticket = 0;
+		atomic_set(&sem->serving_ticket, 0);
+	}
+
+        /* we lose the benefit of priority boosting */
+        unboost_priority(t);
+
+        /* check if there are jobs waiting for this resource */
+        next_candi = __check_waitqueue_first(&sem->wait);
+	if (next_candi) {
+		task_order = get_rt_order(next_candi);
+	}
+	if (task_order == next_serving_ticket) {
+		next = __waitqueue_remove_first(&sem->wait);
+		sem->owner = next;
+	} else {
+		sem->owner = NULL;
+	}
+
+out:
+	TRACE_CUR("Unlock: task order is now %d and semaphore serving is %d on processor %d\n", task_order, next_serving_ticket, get_partition(t));
+        spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+        /* Wake up next. The waiting job is already priority-boosted. */
+        if(next) {
+                wake_up_process(next);
+        }
+
+        preempt_enable();
+
+        return err;
+}
+
+int pfp_pdga_open(struct litmus_lock* l, void* config)
+{
+        struct task_struct *t = current;
+        int cpu, local_cpu;
+        struct pdga_semaphore *sem = pdga_from_lock(l);
+        unsigned long flags;
+
+        if (!is_realtime(t))
+                /* we need to know the real-time priority */
+                return -EPERM;
+
+        local_cpu = get_partition(t);
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+        for (cpu = 0; cpu < NR_CPUS; cpu++) {
+                if (cpu != local_cpu) {
+                        sem->prio_ceiling[cpu] = min(sem->prio_ceiling[cpu],
+                                                     get_priority(t));
+                        TRACE_CUR("priority ceiling for sem %p is now %d on cpu %d\n",
+                                  sem, sem->prio_ceiling[cpu], cpu);
+                }
+        }
+        spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+        return 0;
+}
+
+int pfp_pdga_close(struct litmus_lock* l)
+{
+        struct task_struct *t = current;
+        struct pdga_semaphore *sem = pdga_from_lock(l);
+        unsigned long flags;
+
+        int owner;
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+
+        owner = sem->owner == t;
+
+        spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+        if (owner)
+                pfp_pdga_unlock(l);
+
+        return 0;
+}
+
+void pfp_pdga_free(struct litmus_lock* lock)
+{
+        kfree(pdga_from_lock(lock));
+}
+
+static struct litmus_lock_ops pfp_pdga_lock_ops = {
+        .close  = pfp_pdga_close,
+        .lock   = pfp_pdga_lock,
+        .open   = pfp_pdga_open,
+        .unlock = pfp_pdga_unlock,
+        .deallocate = pfp_pdga_free,
+};
+
+static struct litmus_lock* pfp_new_pdga(void)
+{
+        struct pdga_semaphore* sem;
+        int cpu;
+
+        sem = kmalloc(sizeof(*sem), GFP_KERNEL);
+        if (!sem)
+                return NULL;
+
+        sem->owner = NULL;
+        init_waitqueue_head(&sem->wait);
+        sem->litmus_lock.ops = &pfp_pdga_lock_ops;
+	atomic_set(&sem->serving_ticket, 0);
+
+        for (cpu = 0; cpu < NR_CPUS; cpu++)
+                sem->prio_ceiling[cpu] = OMEGA_CEILING;
+
+        return &sem->litmus_lock;
+}
+
+/* ******************** SDGA support ********************** */
+
+struct sdga_semaphore {
+        struct litmus_lock litmus_lock;
+
+        /* current resource holder */
+        struct task_struct *owner;
+        int owner_cpu;
+
+        /* priority queue of waiting tasks */
+        wait_queue_head_t wait;
+
+        /* where is the resource assigned to */
+        int on_cpu;
+
+	/* current serving task to follow the predefined order */
+	atomic_t serving_ticket;
+};
+
+static inline struct sdga_semaphore* sdga_from_lock(struct litmus_lock* lock)
+{
+        return container_of(lock, struct sdga_semaphore, litmus_lock);
+}
+
+int pfp_sdga_lock(struct litmus_lock* l)
+{
+        struct task_struct* t = current;
+        struct sdga_semaphore *sem = sdga_from_lock(l);
+        int from  = get_partition(t);
+        int to    = sem->on_cpu;
+        unsigned long flags;
+	
+	unsigned int current_serving_ticket;
+	unsigned int task_order;
+
+        prio_wait_queue_t wait;
+        task_order = get_rt_order(t);
+
+        if (!is_realtime(t))
+                return -EPERM;
+
+        /* prevent nested lock accquisition */
+        if (tsk_rt(t)->num_locks_held ||
+            tsk_rt(t)->num_local_locks_held)
+                return -EBUSY;
+
+        preempt_disable();
+
+        /* Priority-boost ourself *before* we suspend so that
+         * our priority is boosted when we resume. */
+        boost_priority(t, task_order);
+
+        pfp_migrate_to(to);
+
+        /* Now on the right CPU, preemptions still disabled. */
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+	
+	current_serving_ticket = atomic_read(&sem->serving_ticket);
+
+        if (sem->owner || current_serving_ticket != task_order) {
+                /* resource is not free => must suspend and wait */
+
+                init_prio_waitqueue_entry(&wait, t, prio_point(task_order));
+
+                /* FIXME: interruptible would be nice some day */
+                set_task_state(t, TASK_UNINTERRUPTIBLE);
+
+                __add_wait_queue_prio_exclusive(&sem->wait, &wait);
+
+                TS_LOCK_SUSPEND;
+
+                /* release lock before sleeping */
+                spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+                /* We depend on the FIFO order.  Thus, we don't need to recheck
+                 * when we wake up; we are guaranteed to have the lock since
+                 * there is only one wake up per release.
+                 */
+
+                preempt_enable_no_resched();
+
+                schedule();
+
+                preempt_disable();
+
+                TS_LOCK_RESUME;
+               /* Since we hold the lock, no other task will change
+                 * ->owner. We can thus check it without acquiring the spin
+                 * lock. */
+                BUG_ON(sem->owner != t);
+        } else {
+                /* it's ours now */
+                sem->owner = t;
+
+                spin_unlock_irqrestore(&sem->wait.lock, flags);
+        }
+
+        sem->owner_cpu = from;
+
+        preempt_enable();
+
+        tsk_rt(t)->num_locks_held++;
+
+        return 0;
+}
+
+int pfp_sdga_unlock(struct litmus_lock* l)
+{
+        struct task_struct *t = current;
+	struct task_struct *next = NULL;
+	struct task_struct *next_candi = NULL;
+        struct sdga_semaphore *sem = sdga_from_lock(l);
+        int err = 0;
+        int home;
+        unsigned long flags;
+
+	unsigned int next_serving_ticket = -1;
+	unsigned int task_order = -1;
+	unsigned int max_serving_ticket = -1;
+
+        preempt_disable();
+
+        spin_lock_irqsave(&sem->wait.lock, flags);
+
+        if (sem->owner != t) {
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sem->wait.lock, flags);
+                goto out;
+        }
+
+	max_serving_ticket = get_rt_total(t) - 1;
+	tsk_rt(t)->num_locks_held--;
+	
+	atomic_add(1, &sem->serving_ticket);
+	next_serving_ticket = atomic_read(&sem->serving_ticket);
+
+	/* if outof range means one iteration has finished */
+	if (next_serving_ticket > max_serving_ticket) {
+		next_serving_ticket = 0;
+		atomic_set(&sem->serving_ticket, 0);
+	}
+
+        /* check if there are jobs waiting for this resource */
+        next_candi = __check_waitqueue_first(&sem->wait);
+
+        if (next_candi) {
+		task_order = get_rt_order(next_candi);
+        }
+
+	if (task_order == next_serving_ticket) {
+		next = __waitqueue_remove_first(&sem->wait);
+		sem->owner = next;
+	} else {
+		sem->owner = NULL;
+	}
+
+        home = sem->owner_cpu;
+
+        spin_unlock_irqrestore(&sem->wait.lock, flags);
+
+	if (next) {
+		wake_up_process(next);
+	}
+
+        /* we lose the benefit of priority boosting */
+        unboost_priority(t);
+
+        pfp_migrate_to(home);
+
+out:
+        preempt_enable();
+
+        return err;
+}
+
+int pfp_sdga_open(struct litmus_lock* l, void* __user config)
+{
+        struct sdga_semaphore *sem = sdga_from_lock(l);
+        int cpu;
+
+        if (get_user(cpu, (int*) config))
+                return -EFAULT;
+
+        /* make sure the resource location matches */
+        if (cpu != sem->on_cpu)
+                return -EINVAL;
+
+        return 0;
+}
+
+int pfp_sdga_close(struct litmus_lock* l)
+{
+        struct task_struct *t = current;
+        struct sdga_semaphore *sem = sdga_from_lock(l);
+        int owner = 0;
+
+        preempt_disable();
+
+        if (sem->on_cpu == smp_processor_id())
+                owner = sem->owner == t;
+
+        preempt_enable();
+
+        if (owner)
+                pfp_sdga_unlock(l);
+
+        return 0;
+}
+
+void pfp_sdga_free(struct litmus_lock* lock)
+{
+        kfree(sdga_from_lock(lock));
+}
+
+static struct litmus_lock_ops pfp_sdga_lock_ops = {
+        .close  = pfp_sdga_close,
+        .lock   = pfp_sdga_lock,
+        .open   = pfp_sdga_open,
+        .unlock = pfp_sdga_unlock,
+        .deallocate = pfp_sdga_free,
+};
+
+static struct litmus_lock* pfp_new_sdga(int on_cpu)
+{
+        struct sdga_semaphore* sem;
+
+        sem = kmalloc(sizeof(*sem), GFP_KERNEL);
+        if (!sem)
+                return NULL;
+
+        sem->litmus_lock.ops = &pfp_sdga_lock_ops;
+        sem->owner_cpu = NO_CPU;
+        sem->owner   = NULL;
+        sem->on_cpu  = on_cpu;
+	atomic_set(&sem->serving_ticket,0);
+        init_waitqueue_head(&sem->wait);
+
+        return &sem->litmus_lock;
+}
 
 /* ******************** DFLP support ********************** */
 
@@ -1862,12 +2343,38 @@
 			err = -ENOMEM;
 		break;
 
+        case SDGA_SEM:
+                /* Semi-partitioned Dependency Graph Approach */
+                if (get_user(cpu, (int*) config))
+                        return -EFAULT;
+
+                TRACE("SDGA_SEM: provided cpu=%d\n", cpu);
+
+                if (cpu >= NR_CPUS || !cpu_online(cpu))
+                        return -EINVAL;
+
+                *lock = pfp_new_sdga(cpu);
+                if (*lock)
+                        err = 0;
+                else
+                        err = -ENOMEM;
+                break;
+
+        case PDGA_SEM:
+                /* Partitioned Dependency Graph Approach */
+                *lock = pfp_new_pdga();
+                if (*lock)
+                        err = 0;
+                else
+                        err = -ENOMEM;
+                break;
+
 	case DFLP_SEM:
 		/* Distributed FIFO Locking Protocol */
 		if (get_user(cpu, (int*) config))
 			return -EFAULT;
 
-		TRACE("DPCP_SEM: provided cpu=%d\n", cpu);
+		TRACE("DFLP_SEM: provided cpu=%d\n", cpu);
 
 		if (cpu >= NR_CPUS || !cpu_online(cpu))
 			return -EINVAL;
